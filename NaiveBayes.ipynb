{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Los hombres no lloran</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estoy enamorado de cuatro babies</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lo mueve rico</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ya no sé que hacer No sé con cuál quedarme Tod...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dos son casadas hay una soltera La otra medio ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texto  Categoria\n",
       "0                              Los hombres no lloran          2\n",
       "1                   Estoy enamorado de cuatro babies          3\n",
       "2                                      Lo mueve rico          3\n",
       "3  Ya no sé que hacer No sé con cuál quedarme Tod...          3\n",
       "4  Dos son casadas hay una soltera La otra medio ...          3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('sexismo.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    10\n",
       "2     1\n",
       "Name: Categoria, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Categoria'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'es', 'una', 'herramienta', 'de', 'comunicación', 'muy', 'bueno', '.', 'Todo', 'esto', 'es', 'gracias', 'al', 'esfuerzo', 'de', 'todos', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#dividir la oración en palabras\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data = \"Facebook es una herramienta de comunicación muy bueno. Todo esto es gracias al esfuerzo de todos.\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estuviesen', 'uno', 'que', 'este', 'otra', 'nuestros', 'su', 'hasta', 'hubierais', 'hubieses', 'seré', 'de', 'antes', 'fueron', 'estad', 'fueran', 'ti', 'fuésemos', 'el', 'fuiste', 'soy', 'estaréis', 'habido', 'habidos', 'he', 'está', 'nosotros', 'estábamos', 'habrá', 'estadas', 'del', 'tenidas', 'tendrías', 'le', 'teniendo', 'ha', 'tuyas', 'estuviste', 'seamos', 'seremos', 'esa', 'tendrían', 'suyos', 'tendrá', 'sentida', 'habíamos', 'tuvieseis', 'han', 'al', 'fuisteis', 'esta', 'tenía', 'cual', 'no', 'estados', 'tuyo', 'tuvieron', 'habida', 'por', 'hubieras', 'todo', 'tengas', 'erais', 'quien', 'te', 'estuvieran', 'hube', 'tuvimos', 'os', 'ellos', 'vosotros', 'hayamos', 'serás', 'hubieron', 'sentidos', 'tuvisteis', 'hemos', 'esos', 'la', 'tuya', 'hubiera', 'tendrán', 'sería', 'tuvierais', 'también', 'eres', 'hubiste', 'estarás', 'otro', 'nuestra', 'serías', 'estuvisteis', 'más', 'tened', 'a', 'habrían', 'él', 'todos', 'estaban', 'tengo', 'habrías', 'estaba', 'habré', 'unos', 'estado', 'sobre', 'fuerais', 'tendría', 'teníais', 'habríamos', 'los', 'mí', 'tenidos', 'sentidas', 'sois', 'fue', 'habréis', 'tuviesen', 'fuera', 'tus', 'entre', 'fuese', 'estemos', 'vuestra', 'tendrás', 'habías', 'era', 'estés', 'fui', 'cuando', 'o', 'mis', 'es', 'serán', 'pero', 'teníamos', 'tenéis', 'tuviese', 'estas', 'estáis', 'ese', 'míos', 'hayas', 'estuvieras', 'algo', 'tuve', 'seríais', 'esté', 'algunas', 'nosotras', 'eran', 'sin', 'estoy', 'tenido', 'serían', 'tuviéramos', 'una', 'tuvieras', 'fueseis', 'mía', 'ya', 'estaré', 'suya', 'tuyos', 'mío', 'las', 'tú', 'eso', 'estuvo', 'muchos', 'estaríais', 'estuve', 'vuestros', 'nada', 'tengan', 'estéis', 'ellas', 'suyo', 'sintiendo', 'estaremos', 'donde', 'habían', 'seas', 'muy', 'habíais', 'tuvieran', 'fuesen', 'tendréis', 'habrás', 'tuviera', 'tanto', 'habremos', 'tuvieses', 'estuviéramos', 'están', 'qué', 'otras', 'hayáis', 'y', 'estará', 'estar', 'sea', 'estás', 'tendremos', 'mías', 'ante', 'nuestras', 'siente', 'estando', 'hayan', 'hubiéramos', 'estuvieron', 'vuestras', 'tengamos', 'nos', 'hubieran', 'en', 'estabas', 'tenga', 'les', 'estos', 'habrán', 'tendré', 'estuvimos', 'me', 'algunos', 'estaría', 'fueras', 'has', 'hubisteis', 'estuviera', 'tengáis', 'estamos', 'quienes', 'hubiese', 'fuimos', 'éramos', 'desde', 'seáis', 'tenían', 'estabais', 'contra', 'para', 'seríamos', 'estuviésemos', 'habría', 'mi', 'tenida', 'tendríamos', 'tienen', 'tendríais', 'sí', 'son', 'sean', 'tenemos', 'mucho', 'habéis', 'hubiésemos', 'tiene', 'como', 'vuestro', 'será', 'tuvo', 'tienes', 'seréis', 'haya', 'fueses', 'otros', 'estuvieses', 'hubieseis', 'estuviese', 'esto', 'con', 'eras', 'yo', 'durante', 'ella', 'fuéramos', 'tuviste', 'tenías', 'poco', 'un', 'se', 'suyas', 'habríais', 'hubimos', 'somos', 'tuviésemos', 'lo', 'estarías', 'ni', 'hay', 'estuvierais', 'estarían', 'sus', 'estén', 'e', 'esas', 'porque', 'estada', 'tu', 'hubo', 'habiendo', 'sentido', 'estaríamos', 'estarán', 'habidas', 'hubiesen', 'había', 'estuvieseis', 'sentid', 'vosotras', 'nuestro'}\n"
     ]
    }
   ],
   "source": [
    "#Se realia el procesamiento de lenguaje natural\n",
    "# y se borra palabras innecesarias como a, es, un,\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords= set(stopwords.words('spanish'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'a', 'c', 'e', 'b', 'o', 'o', 'k', ' ', 'e', 's', ' ', 'u', 'n', 'a', ' ', 't', 'e', 'c', 'n', 'o', 'l', 'o', 'g', 'í', 'a', 'F', 'a', 'c', 'e', 'b', 'o', 'o', 'k', ' ', 'e', 's', ' ', 'g', 'e', 'n', 'i', 'a', 'l', ' ']\n"
     ]
    }
   ],
   "source": [
    "#el modulo string nos permite eliminar signos de puntuacion \n",
    "import string \n",
    "data = \"Facebook es una tecnología,Facebook es genial !.\"\n",
    "data_1=[char for char in data if char not in string.punctuation]\n",
    "print(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook es una tecnologíaFacebook es genial \n"
     ]
    }
   ],
   "source": [
    "#aqui formulamos de nuevo la horacion sin los signos de puntuación\n",
    "data_1= ''.join(data_1)\n",
    "print(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'es', 'una', 'tecnologíaFacebook', 'es', 'genial']\n"
     ]
    }
   ],
   "source": [
    "data_1=data_1.split()\n",
    "print(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facebook': 1, 'es': 0, 'una': 4, 'tecnología': 3, 'genial': 2}\n"
     ]
    }
   ],
   "source": [
    "#vectorizador de conteo\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "data_1 = [\"Facebook es una tecnología Facebook es genial\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(data_1)\n",
    "\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2\n",
      "  (0, 1)\t2\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n"
     ]
    }
   ],
   "source": [
    "#la columna de la izquierda indica el numero de fila de la oracion, en este caso solo tenemos una fila por lo\n",
    "#equivale a 0, la siguiente columna el valor asignado a la palabra, y la ultima es las veces que se repite\n",
    "#la palabra\n",
    "\"\"\"encode document\"\"\"\n",
    "vector = vectorizer.transform(data_1)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(a):\n",
    "  remove_punctuation = [char for char in a if char not in string.punctuation]\n",
    "  remove_punctuation = ''.join(remove_punctuation)\n",
    "  return [word for word in remove_punctuation.split() if word.lower() not in stopwords.words('spanish')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Los hombres no lloran</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estoy enamorado de cuatro babies</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lo mueve rico</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ya no sé que hacer No sé con cuál quedarme Tod...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dos son casadas hay una soltera La otra medio ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texto  Categoria\n",
       "0                              Los hombres no lloran          2\n",
       "1                   Estoy enamorado de cuatro babies          3\n",
       "2                                      Lo mueve rico          3\n",
       "3  Ya no sé que hacer No sé con cuál quedarme Tod...          3\n",
       "4  Dos son casadas hay una soltera La otra medio ...          3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                     [hombres, lloran]\n",
      "1                           [enamorado, cuatro, babies]\n",
      "2                                         [mueve, rico]\n",
      "3     [sé, hacer, sé, cuál, quedarme, Todas, saben, ...\n",
      "4     [Dos, casadas, soltera, medio, psycho, si, lla...\n",
      "5     [primera, desespera, encojona, si, echo, afuer...\n",
      "6     [pelirroja, chichando, moja, encojona, llame, ...\n",
      "7     [enamorado, cuatro, babys, Siempre, dan, quier...\n",
      "8     [Cuatro, chimbitas, Cuatro, personalidades, Do...\n",
      "9     [Quieren, lleve, pa, Medallo, Quieren, monte, ...\n",
      "10    [Prende, phillie, bebé, mismo, apaga, Vamos, c...\n",
      "Name: Texto, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[:,0].apply(text_cleaning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hombres': 53,\n",
       " 'lloran': 61,\n",
       " 'enamorado': 47,\n",
       " 'cuatro': 37,\n",
       " 'babies': 19,\n",
       " 'mueve': 68,\n",
       " 'rico': 85,\n",
       " 'sé': 92,\n",
       " 'hacer': 52,\n",
       " 'cuál': 38,\n",
       " 'quedarme': 80,\n",
       " 'Todas': 13,\n",
       " 'saben': 87,\n",
       " 'cama': 25,\n",
       " 'maltratarme': 63,\n",
       " 'Dos': 4,\n",
       " 'casadas': 27,\n",
       " 'soltera': 91,\n",
       " 'medio': 64,\n",
       " 'psycho': 79,\n",
       " 'si': 89,\n",
       " 'llamo': 59,\n",
       " 'desespera': 40,\n",
       " 'primera': 78,\n",
       " 'encojona': 48,\n",
       " 'echo': 46,\n",
       " 'afueraLa': 15,\n",
       " 'segunda': 88,\n",
       " 'funda': 50,\n",
       " 'paga': 72,\n",
       " 'pa': 71,\n",
       " 'hunda': 54,\n",
       " 'tercera': 93,\n",
       " 'quita': 83,\n",
       " 'estrés': 49,\n",
       " 'Polvos': 9,\n",
       " 'corridos': 33,\n",
       " 'siempre': 90,\n",
       " 'echamos': 45,\n",
       " 'tres': 95,\n",
       " 'pelirroja': 73,\n",
       " 'chichando': 28,\n",
       " 'moja': 66,\n",
       " 'llame': 58,\n",
       " 'coja': 32,\n",
       " 'Peleamos': 8,\n",
       " 'bota': 24,\n",
       " 'ropa': 86,\n",
       " 'llamar': 57,\n",
       " 'cotorra': 35,\n",
       " 'recoja': 84,\n",
       " 'chiquitita': 31,\n",
       " 'nalgona': 70,\n",
       " 'pelo': 74,\n",
       " 'corto': 34,\n",
       " 'babys': 20,\n",
       " 'Siempre': 12,\n",
       " 'dan': 39,\n",
       " 'quiero': 82,\n",
       " 'Chingan': 0,\n",
       " 'digo': 42,\n",
       " 'Ninguna': 7,\n",
       " 'pone': 77,\n",
       " 'Cuatro': 1,\n",
       " 'chimbitas': 29,\n",
       " 'personalidades': 75,\n",
       " 'hablan': 51,\n",
       " 'bonito': 23,\n",
       " 'dicen': 41,\n",
       " 'maldades': 62,\n",
       " 'Diferentes': 2,\n",
       " 'nacionalidades': 69,\n",
       " 'chingan': 30,\n",
       " 'Gritan': 5,\n",
       " 'todas': 94,\n",
       " 'iguales': 55,\n",
       " 'Quieren': 11,\n",
       " 'lleve': 60,\n",
       " 'Medallo': 6,\n",
       " 'monte': 67,\n",
       " 'carro': 26,\n",
       " 'año': 18,\n",
       " 'apriete': 17,\n",
       " 'dos': 43,\n",
       " 'dé': 44,\n",
       " 'juntas': 56,\n",
       " 'baño': 21,\n",
       " 'Digan': 3,\n",
       " 'quieren': 81,\n",
       " 'Prende': 10,\n",
       " 'phillie': 76,\n",
       " 'bebé': 22,\n",
       " 'mismo': 65,\n",
       " 'apaga': 16,\n",
       " 'Vamos': 14,\n",
       " 'cuarto': 36}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer=text_cleaning).fit(df['Texto'])\n",
    "\n",
    "bow_transformer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= bow_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(CountVectorizer(analyzer=<function text_cleaning at 0x000002A1E96E88B0>),\n      dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-4aa6f90c7c4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2125\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2127\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \"\"\"\n\u001b[0;32m    292\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0m\u001b[0;32m    197\u001b[0m                             \" a valid collection.\" % x)\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(CountVectorizer(analyzer=<function text_cleaning at 0x000002A1E96E88B0>),\n      dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 53)\t1\n",
      "  (0, 61)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 37)\t1\n",
      "  (1, 47)\t1\n",
      "  (2, 68)\t1\n",
      "  (2, 85)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 25)\t1\n",
      "  (3, 38)\t1\n",
      "  (3, 52)\t1\n",
      "  (3, 63)\t1\n",
      "  (3, 80)\t1\n",
      "  (3, 87)\t1\n",
      "  (3, 92)\t2\n",
      "  (4, 4)\t1\n",
      "  (4, 27)\t1\n",
      "  (4, 40)\t1\n",
      "  (4, 59)\t1\n",
      "  (4, 64)\t1\n",
      "  (4, 79)\t1\n",
      "  (4, 89)\t1\n",
      "  (4, 91)\t1\n",
      "  (5, 9)\t1\n",
      "  (5, 15)\t1\n",
      "  :\t:\n",
      "  (8, 75)\t1\n",
      "  (8, 94)\t1\n",
      "  (9, 3)\t1\n",
      "  (9, 6)\t1\n",
      "  (9, 11)\t2\n",
      "  (9, 17)\t1\n",
      "  (9, 18)\t1\n",
      "  (9, 21)\t1\n",
      "  (9, 26)\t1\n",
      "  (9, 32)\t1\n",
      "  (9, 43)\t1\n",
      "  (9, 44)\t1\n",
      "  (9, 52)\t1\n",
      "  (9, 56)\t1\n",
      "  (9, 60)\t1\n",
      "  (9, 67)\t1\n",
      "  (9, 71)\t1\n",
      "  (9, 81)\t1\n",
      "  (10, 10)\t1\n",
      "  (10, 14)\t1\n",
      "  (10, 16)\t1\n",
      "  (10, 22)\t1\n",
      "  (10, 36)\t1\n",
      "  (10, 65)\t1\n",
      "  (10, 76)\t1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "title_bow = bow_transformer.transform(df['Texto'])\n",
    "\n",
    "print(title_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer()\n",
      "  (0, 61)\t0.7071067811865476\n",
      "  (0, 53)\t0.7071067811865476\n",
      "  (1, 47)\t0.5448398854754147\n",
      "  (1, 37)\t0.5448398854754147\n",
      "  (1, 19)\t0.6374158755398818\n",
      "  (2, 85)\t0.7071067811865476\n",
      "  (2, 68)\t0.7071067811865476\n",
      "  (3, 92)\t0.6105448490317061\n",
      "  (3, 87)\t0.30527242451585307\n",
      "  (3, 80)\t0.30527242451585307\n",
      "  (3, 63)\t0.30527242451585307\n",
      "  (3, 52)\t0.26093575512399203\n",
      "  (3, 38)\t0.30527242451585307\n",
      "  (3, 25)\t0.30527242451585307\n",
      "  (3, 13)\t0.30527242451585307\n",
      "  (4, 91)\t0.37288877911826523\n",
      "  (4, 89)\t0.31873175348478006\n",
      "  (4, 79)\t0.37288877911826523\n",
      "  (4, 64)\t0.37288877911826523\n",
      "  (4, 59)\t0.37288877911826523\n",
      "  (4, 40)\t0.31873175348478006\n",
      "  (4, 27)\t0.37288877911826523\n",
      "  (4, 4)\t0.31873175348478006\n",
      "  (5, 95)\t0.2373099407558509\n",
      "  (5, 93)\t0.2373099407558509\n",
      "  :\t:\n",
      "  (8, 2)\t0.22988515942758447\n",
      "  (8, 1)\t0.45977031885516895\n",
      "  (9, 81)\t0.23553012693103106\n",
      "  (9, 71)\t0.1770519359483702\n",
      "  (9, 67)\t0.23553012693103106\n",
      "  (9, 60)\t0.23553012693103106\n",
      "  (9, 56)\t0.23553012693103106\n",
      "  (9, 52)\t0.2013225780961644\n",
      "  (9, 44)\t0.23553012693103106\n",
      "  (9, 43)\t0.23553012693103106\n",
      "  (9, 32)\t0.2013225780961644\n",
      "  (9, 26)\t0.23553012693103106\n",
      "  (9, 21)\t0.23553012693103106\n",
      "  (9, 18)\t0.23553012693103106\n",
      "  (9, 17)\t0.23553012693103106\n",
      "  (9, 11)\t0.47106025386206213\n",
      "  (9, 6)\t0.23553012693103106\n",
      "  (9, 3)\t0.23553012693103106\n",
      "  (10, 76)\t0.37796447300922725\n",
      "  (10, 65)\t0.37796447300922725\n",
      "  (10, 36)\t0.37796447300922725\n",
      "  (10, 22)\t0.37796447300922725\n",
      "  (10, 16)\t0.37796447300922725\n",
      "  (10, 14)\t0.37796447300922725\n",
      "  (10, 10)\t0.37796447300922725\n",
      "(11, 96)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer().fit(title_bow)\n",
    "print(tfidf_transformer)\n",
    "\n",
    "title_tfidf=tfidf_transformer.transform(title_bow)\n",
    "print(title_tfidf)\n",
    "print(title_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(title_tfidf,df['Categoría'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sexismo de categoria C' 'Sexismo de categoria C'\n",
      " 'Sexismo de categoria C' 'Sexismo de categoria C'\n",
      " 'Sexismo de categoria C' 'Sexismo de categoria C'\n",
      " 'Sexismo de categoria C' 'Sexismo de categoria C'\n",
      " 'Sexismo de categoria C' 'Sexismo de categoria C'\n",
      " 'Sexismo de categoria C']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = model.predict(title_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-efc5993c4d78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconfusion_matrix_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Categoría'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Categoría'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix_report' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_report(df['Categoría'], all_predictions)\n",
    "confusion_matrix(df['Categoría'], all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
